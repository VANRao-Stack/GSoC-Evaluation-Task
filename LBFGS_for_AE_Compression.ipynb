{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LBFGS for AE Compression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAwPAAEdGJEY"
      },
      "source": [
        "**Testing L-BFGS Optimizer**\r\n",
        "\r\n",
        "The original thesis by Eric Wulff primarily investigated the usage of the Adam optimizer with weight decay for the purposse of training the Autoencoder. In this notebook I investigate the usage of the L-BFGS optimizer to achieve the same. It should however be noted that I have not performed any extensive research on the same, and hence this method naturally warrants more investigation, the importance of which will be determined in this notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaVZuG2yJYzK"
      },
      "source": [
        "**Importing and preparing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCOXKj1JqkTp"
      },
      "source": [
        "#Import necesary libraries\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9udoeTO3qpK6"
      },
      "source": [
        "#Attach the csv files prepared in the previous notebook and import then as DataFrames\r\n",
        "gauss_x = pd.read_csv('/Dataset/Rank Gauss.csv')\r\n",
        "standard_x = pd.read_csv('/Dataset/Standard.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXkHS_CkqtcR"
      },
      "source": [
        "#Split the datasets into the train, validation and test sets\r\n",
        "gauss_x_train, gauss_x_test, standard_x_train, standard_x_test = train_test_split(gauss_x, standard_x, train_size=0.8,test_size=0.2, random_state=42)\r\n",
        "gauss_x_train, gauss_x_valid, standard_x_train, standard_x_valid = train_test_split(gauss_x_train, standard_x_train, train_size=0.8,test_size=0.2, random_state=101)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCyf1BLhqxez"
      },
      "source": [
        "train_standard = standard_x_train\r\n",
        "test_standard = standard_x_valid\r\n",
        "train_gauss = gauss_x_train\r\n",
        "test_gauss = gauss_x_valid"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkYjE5Oaqz_c",
        "outputId": "c756b6a1-885d-4e55-a6c7-2f064c9dc49d"
      },
      "source": [
        "train_standard.pop('Unnamed: 0')\r\n",
        "test_standard.pop('Unnamed: 0')\r\n",
        "train_gauss.pop('Unnamed: 0')\r\n",
        "test_gauss.pop('Unnamed: 0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5118    5118\n",
              "3626    3626\n",
              "869      869\n",
              "2877    2877\n",
              "4460    4460\n",
              "        ... \n",
              "946      946\n",
              "5583    5583\n",
              "4811    4811\n",
              "649      649\n",
              "5561    5561\n",
              "Name: Unnamed: 0, Length: 993, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVNf8pRq2tc"
      },
      "source": [
        "#We have to manually convert the datasets to floats from doubles for training\r\n",
        "for i in train_standard.columns:\r\n",
        "  train_standard[i] = train_standard[i].astype('float32')\r\n",
        "for i in test_standard.columns:\r\n",
        "  test_standard[i] = test_standard[i].astype('float32')\r\n",
        "for i in train_gauss.columns:\r\n",
        "  train_gauss[i] = train_gauss[i].astype('float32')\r\n",
        "for i in test_gauss.columns:\r\n",
        "  test_gauss[i] = test_gauss[i].astype('float32')\r\n",
        "\r\n",
        "#Save the mean and standard deviation of the datasets for plotting\r\n",
        "train_mean_standard = train_standard.mean()\r\n",
        "train_std_standard = train_standard.std()\r\n",
        "train_mean_gauss = train_gauss.mean()\r\n",
        "train_std_gauss = train_gauss.std()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecx_j8rzq5xX",
        "outputId": "9caaab2a-bbd0-4036-ec90-83cc5b70058a"
      },
      "source": [
        "standard_x_test.pop('Unnamed: 0')\r\n",
        "gauss_x_test.pop('Unnamed: 0')\r\n",
        "\r\n",
        "for i in standard_x_test.columns:\r\n",
        "  standard_x_test[i] = standard_x_test[i].astype('float32')\r\n",
        "for i in gauss_x_test.columns:\r\n",
        "  gauss_x_test[i] = gauss_x_test[i].astype('float32')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PU3gfR4JiIF"
      },
      "source": [
        "**Building the Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBrJMUH2txqd"
      },
      "source": [
        "%matplotlib inline\r\n",
        "\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "from torch.optim import Adam, LBFGS\r\n",
        "\r\n",
        "from fastai import data_block, basic_train, basic_data\r\n",
        "from fastai.callbacks import ActivationStats\r\n",
        "from fastai import train as tr\r\n",
        "import fastai\r\n",
        "\r\n",
        "import matplotlib as mpl"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpKJya0Gtevo"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqygcYhwva3V"
      },
      "source": [
        "class AutoEncoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(AutoEncoder, self).__init__()\r\n",
        "        self.en1 = nn.Linear(4, 200)\r\n",
        "        self.en2 = nn.Linear(200, 100)\r\n",
        "        self.en3 = nn.Linear(100, 50)\r\n",
        "        self.en4 = nn.Linear(50, 3)\r\n",
        "        self.de1 = nn.Linear(3, 50)\r\n",
        "        self.de2 = nn.Linear(50, 100)\r\n",
        "        self.de3 = nn.Linear(100, 200)\r\n",
        "        self.de4 = nn.Linear(200, 4)\r\n",
        "        self.tanh = nn.Tanh()\r\n",
        "\r\n",
        "    def encode(self, x):\r\n",
        "        x = self.en1(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.en2(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.en3(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.en4(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def decode(self, x):\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.de1(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.de2(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.de3(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        x = self.de4(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        z = self.encode(x)\r\n",
        "        return self.decode(z)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al8HEXg_uAl8"
      },
      "source": [
        "class AE_LBFGS(AutoEncoder):\r\n",
        "  def __init__(self, loss):\r\n",
        "    AutoEncoder.__init__(self)\r\n",
        "    self.lossFct = loss \r\n",
        "    self.optim = None\r\n",
        "\r\n",
        "  def train_lbfgs(self, data_loader, epochs, validation_data=None):\r\n",
        "    for epoch in range(epochs):\r\n",
        "      running_loss = self._train_iteration(data_loader)\r\n",
        "      val_loss = None\r\n",
        "      if validation_data is not None:\r\n",
        "        y_hat = self(validation_data['X'])\r\n",
        "        val_loss = self.lossFct(input=y_hat, target=validation_data['y']).detach().numpy()\r\n",
        "        print('{}/{} Epoch | loss: {:E} | validation loss: {:E}'.format(str(epoch + 1), str(epochs), running_loss, val_loss))\r\n",
        "      else:\r\n",
        "        print('[%d] loss: %.3f' % (epoch + 1, running_loss))\r\n",
        "\r\n",
        "\r\n",
        "  def _train_iteration(self,data_loader):\r\n",
        "        running_loss = 0.0\r\n",
        "        for i, (X,y) in enumerate(data_loader):\r\n",
        "            X = X.float()\r\n",
        "            y = y.unsqueeze(1).float()\r\n",
        "            X_ = Variable(X, requires_grad=True)\r\n",
        "            y_ = Variable(y)\r\n",
        "            ### Comment out the typical gradient calculation\r\n",
        "#             pred = self(X)\r\n",
        "#             loss = self.lossFct(pred, y)\r\n",
        "#             self.optim.zero_grad()\r\n",
        "#             loss.backward()\r\n",
        "            ### Add the closure function to calculate the gradient.\r\n",
        "            def closure():\r\n",
        "                if torch.is_grad_enabled():\r\n",
        "                    self.optim.zero_grad()\r\n",
        "                output = self(X_)\r\n",
        "                loss = self.lossFct(output, y_)\r\n",
        "                if loss.requires_grad:\r\n",
        "                    loss.backward()\r\n",
        "                return loss\r\n",
        "            self.optim.step(closure)\r\n",
        "            \r\n",
        "            # calculate the loss again for monitoring\r\n",
        "            output = self(X_)\r\n",
        "            loss = closure()\r\n",
        "            running_loss += loss.item()\r\n",
        "               \r\n",
        "        return running_loss\r\n",
        "\r\n",
        "  def predict(self, X):\r\n",
        "    X = torch.Tensor(X)\r\n",
        "    return self(X).detach().numpy().squeeze()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud4ioDEYxINf"
      },
      "source": [
        "class ExperimentData(Dataset):\r\n",
        "    def __init__(self, X, y):\r\n",
        "        self.X = X\r\n",
        "        self.y = y\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return self.X.shape[0]\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.X[idx,:], self.y[idx]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbAxAkE8LAXL"
      },
      "source": [
        "**Training on the Standard Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvPFRztyx9M3"
      },
      "source": [
        "X_standard = np.asarray(train_standard)\r\n",
        "y_standard = np.asarray(test_standard)\r\n",
        "\r\n",
        "train_data_standard = ExperimentData(X_standard, X_standard)\r\n",
        "valid_data_standard = ExperimentData(y_standard, y_standard)\r\n",
        "\r\n",
        "INPUT_SIZE = 4\r\n",
        "EPOCHS=30\r\n",
        "\r\n",
        "pred_val_standard = {}\r\n",
        "data_loader_train_standard = DataLoader(train_data_standard, batch_size=X_standard.shape[0])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22nKksnizCYA"
      },
      "source": [
        "net_standard = AE_LBFGS(loss = nn.MSELoss())\r\n",
        "net_standard.optim = LBFGS(net_standard.parameters(), history_size=10, max_iter=4)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvQSCgUmzsua",
        "outputId": "9db44de8-2a32-459c-fe29-a8a5650521a7"
      },
      "source": [
        "net_standard.train_lbfgs(data_loader_train_standard, EPOCHS, validation_data = {\"X\":torch.Tensor(y_standard), \"y\":torch.Tensor(y_standard).unsqueeze(1) })"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3968, 1, 4])) that is different to the input size (torch.Size([3968, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([993, 1, 4])) that is different to the input size (torch.Size([993, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/30 Epoch | loss: 9.954859E-01 | validation loss: 9.945354E-01\n",
            "2/30 Epoch | loss: 9.954653E-01 | validation loss: 9.945039E-01\n",
            "3/30 Epoch | loss: 9.954565E-01 | validation loss: 9.945714E-01\n",
            "4/30 Epoch | loss: 9.954531E-01 | validation loss: 9.945463E-01\n",
            "5/30 Epoch | loss: 9.954531E-01 | validation loss: 9.945396E-01\n",
            "6/30 Epoch | loss: 9.954517E-01 | validation loss: 9.945116E-01\n",
            "7/30 Epoch | loss: 9.954503E-01 | validation loss: 9.945359E-01\n",
            "8/30 Epoch | loss: 9.954503E-01 | validation loss: 9.945449E-01\n",
            "9/30 Epoch | loss: 9.954498E-01 | validation loss: 9.945412E-01\n",
            "10/30 Epoch | loss: 9.954500E-01 | validation loss: 9.945371E-01\n",
            "11/30 Epoch | loss: 9.954500E-01 | validation loss: 9.945363E-01\n",
            "12/30 Epoch | loss: 9.954500E-01 | validation loss: 9.945353E-01\n",
            "13/30 Epoch | loss: 9.954500E-01 | validation loss: 9.945337E-01\n",
            "14/30 Epoch | loss: 9.954498E-01 | validation loss: 9.945317E-01\n",
            "15/30 Epoch | loss: 9.954498E-01 | validation loss: 9.945366E-01\n",
            "16/30 Epoch | loss: 9.954498E-01 | validation loss: 9.945410E-01\n",
            "17/30 Epoch | loss: 9.954496E-01 | validation loss: 9.945299E-01\n",
            "18/30 Epoch | loss: 9.954495E-01 | validation loss: 9.945362E-01\n",
            "19/30 Epoch | loss: 9.954495E-01 | validation loss: 9.945347E-01\n",
            "20/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945284E-01\n",
            "21/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945356E-01\n",
            "22/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945368E-01\n",
            "23/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945363E-01\n",
            "24/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945357E-01\n",
            "25/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945357E-01\n",
            "26/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945364E-01\n",
            "27/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945364E-01\n",
            "28/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945364E-01\n",
            "29/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945364E-01\n",
            "30/30 Epoch | loss: 9.954492E-01 | validation loss: 9.945364E-01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMT4e3HxTnio"
      },
      "source": [
        "**Training on the Gaussian transformed dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMoeZ0P3BozX"
      },
      "source": [
        "X_gauss = np.asarray(train_gauss)\r\n",
        "y_gauss = np.asarray(test_gauss)\r\n",
        "\r\n",
        "train_data_gauss = ExperimentData(X_gauss, X_gauss)\r\n",
        "valid_data_gauss = ExperimentData(y_gauss, y_gauss)\r\n",
        "\r\n",
        "INPUT_SIZE = 4\r\n",
        "EPOCHS=30\r\n",
        "\r\n",
        "pred_val_gauss = {}\r\n",
        "data_loader_train_gauss = DataLoader(train_data_gauss, batch_size=X_gauss.shape[0])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyNP96UPUG3z"
      },
      "source": [
        "net_gauss = AE_LBFGS(loss = nn.MSELoss())\r\n",
        "net_gauss.optim = LBFGS(net_gauss.parameters(), history_size=10, max_iter=4)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HcNwf2pUOqH",
        "outputId": "0f6ba1c7-0ff3-4683-8060-9b8855fb5ff3"
      },
      "source": [
        "net_gauss.train_lbfgs(data_loader_train_gauss, EPOCHS, validation_data = {\"X\":torch.Tensor(y_gauss), \"y\":torch.Tensor(y_gauss).unsqueeze(1) })"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3968, 1, 4])) that is different to the input size (torch.Size([3968, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([993, 1, 4])) that is different to the input size (torch.Size([993, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/30 Epoch | loss: 9.969780E-01 | validation loss: 9.971191E-01\n",
            "2/30 Epoch | loss: 9.969388E-01 | validation loss: 9.971381E-01\n",
            "3/30 Epoch | loss: 9.969406E-01 | validation loss: 9.971686E-01\n",
            "4/30 Epoch | loss: 9.969358E-01 | validation loss: 9.971371E-01\n",
            "5/30 Epoch | loss: 9.969347E-01 | validation loss: 9.971334E-01\n",
            "6/30 Epoch | loss: 9.969345E-01 | validation loss: 9.971328E-01\n",
            "7/30 Epoch | loss: 9.969338E-01 | validation loss: 9.971284E-01\n",
            "8/30 Epoch | loss: 9.969338E-01 | validation loss: 9.971322E-01\n",
            "9/30 Epoch | loss: 9.969338E-01 | validation loss: 9.971321E-01\n",
            "10/30 Epoch | loss: 9.969336E-01 | validation loss: 9.971314E-01\n",
            "11/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971320E-01\n",
            "12/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971322E-01\n",
            "13/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "14/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "15/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "16/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "17/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "18/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "19/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "20/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "21/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "22/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "23/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "24/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "25/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "26/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "27/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "28/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "29/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n",
            "30/30 Epoch | loss: 9.969335E-01 | validation loss: 9.971323E-01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_dQkYjbwO2k"
      },
      "source": [
        "**Testing**\r\n",
        "\r\n",
        "We now run the two models against each other with the test sets to calculate their final benchmark score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_FuLYmQwNxt"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AjE18qcwUyU"
      },
      "source": [
        "def test(model, device, test_dataloader, plot_type):\r\n",
        "  model.eval()\r\n",
        "  loss = 0\r\n",
        "  success = 0\r\n",
        "  with torch.no_grad():\r\n",
        "    for (X, y) in test_dataloader:\r\n",
        "      X, y = X.to(device), y.to(device)\r\n",
        "      pred_prob = model(X)\r\n",
        "      loss += nn.MSELoss()(pred_prob, y).item()\r\n",
        "  loss /= len(test_dataloader.dataset)\r\n",
        "  print('\\nTest dataset of {}: Overall Loss: {}'.format(plot_type, loss))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNRBdX19wWWb",
        "outputId": "3a6d6d11-8aaf-4ac2-ce29-85aeca58dead"
      },
      "source": [
        "test_ds = TensorDataset(torch.tensor(standard_x_test.values), torch.tensor(standard_x_test.values))\r\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64)\r\n",
        "test(net_standard, 'cpu', test_dataloader, 'Standard model')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test dataset of Standard model: Overall Loss: 0.01638257032820143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTFa8NzowfPW",
        "outputId": "4c6a345d-39aa-465b-adab-f9f896e34f3b"
      },
      "source": [
        "test_ds = TensorDataset(torch.tensor(gauss_x_test.values), torch.tensor(gauss_x_test.values))\r\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64)\r\n",
        "test(net_gauss, 'cpu', test_dataloader, 'Gaussian Model')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test dataset of Gaussian Model: Overall Loss: 0.016301531305628184\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}